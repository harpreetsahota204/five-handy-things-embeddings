{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Audio-to-Image Search Plugin](https://github.com/jacobmarks/audio-retrieval-plugin) is a FiftyOne plugin that allows searching for images similar to a given audio clip. \n",
    "\n",
    "It works by:\n",
    "\n",
    "- Using the `ImageBind` embedding model to embed images and audio clips into a shared 1024-dimensional space.\n",
    "\n",
    "- Storing the embeddings in a `Qdrant` similarity index for fast similarity search.\n",
    "\n",
    "- Providing a FiftyOne UI for uploading audio clips, pre-filtering, and searching the index.\n",
    "\n",
    "The plugin supports `ogg` and `wav` audio files, but not `mp3`. It makes an API call to Replicate to avoid potential installation issues with running the embedding model locally.\n",
    "\n",
    "\n",
    "### ImageBind learns a joint embedding space across multiple modalities\n",
    "\n",
    "\n",
    "ImageBind exploits the natural co-occurrence of images with other modalities to learn a unified representation that binds them together without requiring exhaustive paired data. It uses the frozen CLIP image-text embedding space as the target for alignment. This simple yet powerful approach enables novel multimodal capabilities to emerge.\n",
    "\n",
    "\n",
    "- ImageBind learns a single embedding space that binds together six modalities: images, text, audio, depth, thermal, and IMU data. This allows it to align and connect information from these various sources.\n",
    "\n",
    "- Importantly, ImageBind does not require all combinations of paired data to train this joint embedding. It leverages the fact that images naturally co-occur with the other modalities. By training on image-paired it can implicitly align all the modalities together using the images as an anchor or \"binding\" modality.\n",
    "\n",
    " - Each modality has its own encoder, e.g., a ViT for images, audio spectrograms, depth maps, etc., and a transformer for IMU sequences. A modality-specific linear projection head is added to each encoder to obtain a fixed-dimensional embedding. This embedding is normalized and used in the contrastive InfoNCE loss during training. `This is important, so let me break it down:`\n",
    "\n",
    "    1. **Separate encoders for each modality**:\n",
    "\n",
    "    - ImageBind uses different encoder architectures tailored to each modality. \n",
    "\n",
    "    - For images, audio spectrograms, and depth maps, they employ Vision Transformers (ViT) as the encoder backbone.\n",
    "\n",
    "    - For Inertial Measurement Unit (IMU) sequences, which consist of accelerometer and gyroscope readings over time, they use a standard Transformer encoder.\n",
    "\n",
    "    2. **Modality-specific linear projection heads:**\n",
    "\n",
    "    - The raw output from each modality's encoder may have different dimensionalities.\n",
    "\n",
    "    - To align them into a common embedding space, ImageBind adds a linear projection head on top of each encoder.\n",
    "\n",
    "    - This projection head is just a simple fully-connected layer that maps the encoder output to a fixed target dimensionality.\n",
    "\n",
    "    - The weights of these projection heads are learned during training.\n",
    "\n",
    "    3. **Embedding normalization:**\n",
    "\n",
    "    - After the linear projection, the embeddings from each modality are normalized. \n",
    "\n",
    "    - Normalization here likely refers to L2 normalization, where the embedding vectors are scaled to have unit length.\n",
    "\n",
    "    - Normalization makes the embeddings more comparable and is a common practice in contrastive learning.\n",
    "\n",
    "    4. **InfoNCE contrastive loss:**\n",
    "    - The normalized embeddings from all modalities are used to compute the InfoNCE contrastive loss during training.\n",
    "\n",
    "    - InfoNCE is a popular choice for self-supervised contrastive learning. It encourages embeddings of positive pairs (e.g. an image and its caption) to be close, while pushing apart embeddings of negative pairs.\n",
    "\n",
    "    - By using InfoNCE across embeddings from different modalities, ImageBind learns to align them into a shared semantic space.\n",
    "\n",
    "- The image and text encoders are initialized from a pretrained CLIP model and frozen during training. The other modality encoders are learned to align with this frozen CLIP image-text embedding space.\n",
    "\n",
    "- Through this training on image-paired data, ImageBind implicitly learns to align non-paired modalities, like text and audio. For example, training on (image, text) and (image, audio) pairs enables ImageBind to connect text and audio without seeing them paired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fiftyone as fo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need to install the following plugin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/jacobmarks/audio-retrieval-plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need a Replicate API Token for this notebook. You can sign up [here](https://replicate.com/docs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import getpass\n",
    "\n",
    "# os.environ['REPLICATE_API_TOKEN'] = getpass.getpass(\"Enter your Replicate API token: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FiftyOne has integrations with Hugging Face, which allow you to easily pull datasets from the hub! Learn more about the integration [here](https://docs.voxel51.com/integrations/huggingface.html) and how you can pull datasets from the hub [here](https://docs.voxel51.com/integrations/huggingface.html#loading-datasets-from-the-hub)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fiftyone.utils.huggingface as fouh\n",
    "\n",
    "# instruments_dataset = fouh.load_from_hub(\n",
    "#     \"YakupAkdin/instrument-images\",\n",
    "#     split=\"train\",\n",
    "#     format= \"ParquetFilesDataset\",\n",
    "#     overwrite=True,\n",
    "#     persistent=True,\n",
    "#     name=\"instruments\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I already have the dataset downloaded, so I will just load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruments_dataset = fo.load_dataset(\"instruments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, you can use the `take` method of the dataset which randomly samples the given number of samples from the collection. This will keep costs low for you on Replicate and save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sample = instruments_dataset.take(\n",
    "    size=250, \n",
    "    seed=51, \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code overrides the default operator timeout. This is done because it may take a long time to generate embeddings, as they are coming from the Replicate API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.config.operator_timeout = 6_000_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The audio-retrieval-plugin allows you to search a dataset for images that are similar to a given audio clip. It works by using the `ImageBind` embedding model to embed images and audio clips into a shared \n",
    "\n",
    "To use the plugin, you need to:\n",
    "\n",
    "1. Start a local Qdrant server using Docker: `docker run -p \"6333:6333\" -p \"6334:6334\" -d qdrant/qdrant`\n",
    "\n",
    "2. Run the `create_imagebind_index` operator to create the similarity index\n",
    "\n",
    "3. Run the `open_audio_retrieval_panel` operator to open the search panel\n",
    "\n",
    "The search panel allows you to upload an audio clip (in `ogg` or `wav` format), and then searches the index for similar images. This plugin is a proof of concept and not intended for production use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
