{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To distill this information while keeping the important technical details for deep learning practitioners, I'll focus on the key aspects of the Audio-to-Image Search Plugin and the ImageBind model it utilizes. Here's a concise yet informative version:\n",
    "\n",
    "# Audio-to-Image Search Plugin: Leveraging ImageBind for Multimodal Retrieval\n",
    "\n",
    "## Plugin Overview\n",
    "\n",
    "The [Audio-to-Image Search Plugin](https://github.com/jacobmarks/audio-retrieval-plugin) for FiftyOne enables image retrieval based on audio input. Key features:\n",
    "\n",
    "- Utilizes ImageBind for embedding audio and images into a shared 1024D space\n",
    "- Employs Qdrant for efficient similarity search\n",
    "- Supports `.ogg` and `.wav` audio formats\n",
    "- Uses Replicate API for embedding generation\n",
    "\n",
    "## ImageBind: One embedding space to bind them all!\n",
    "\n",
    "ImageBind creates a joint embedding space for six modalities: images, text, audio, depth, thermal, and IMU data.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Leveraging Natural Co-occurrence**: Exploits image co-occurrence with other modalities to learn unified representations without exhaustive paired data.\n",
    "\n",
    "2. **CLIP-based Alignment**: Uses frozen CLIP image-text embedding space as the target for alignment.\n",
    "\n",
    "3. **Modality-Specific Encoders**:\n",
    "   - Images, audio spectrograms, depth maps: Vision Transformers (ViT)\n",
    "   - IMU sequences: Standard Transformer encoder\n",
    "\n",
    "4. **Embedding Alignment**:\n",
    "   - Modality-specific linear projection heads map encoder outputs to a fixed dimension\n",
    "   - L2 normalization applied to embeddings\n",
    "\n",
    "5. **Training Approach**:\n",
    "   - InfoNCE contrastive loss used across modalities\n",
    "   - Image and text encoders initialized from pretrained CLIP and frozen\n",
    "   - Other modality encoders learned to align with CLIP space\n",
    "\n",
    "6. **Emergent Cross-modal Connections**: e.g., (image, text) and (image, audio) pairs enable text-audio connections without direct pairing\n",
    "\n",
    "This architecture allows ImageBind to create a powerful unified embedding space, enabling novel multimodal applications like the Audio-to-Image Search Plugin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fiftyone as fo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need to install the following plugin for concept space traversal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/jacobmarks/audio-retrieval-plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the following plugin for concept interpolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/jacobmarks/concept-interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need a Replicate API Token for this notebook. You can sign up [here](https://replicate.com/docs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "os.environ['REPLICATE_API_TOKEN'] = getpass.getpass(\"Enter your Replicate API token: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FiftyOne has integrations with Hugging Face, which allow you to easily pull datasets from the hub! Learn more about the integration [here](https://docs.voxel51.com/integrations/huggingface.html) and how you can pull datasets from the hub [here](https://docs.voxel51.com/integrations/huggingface.html#loading-datasets-from-the-hub)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.utils.huggingface as fouh\n",
    "\n",
    "instruments_dataset = fouh.load_from_hub(\n",
    "    \"YakupAkdin/instrument-images\",\n",
    "    split=\"train\",\n",
    "    format= \"ParquetFilesDataset\",\n",
    "    name=\"instruments\",\n",
    "    overwrite=True,\n",
    "    persistent=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If already have the dataset downloaded, so you can just load it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruments_dataset = fo.load_dataset(\"instruments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, you can use the `take` method of the dataset which randomly samples the given number of samples from the collection. This will keep costs low for you on Replicate and save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sample = instruments_dataset.take(\n",
    "    size=250, \n",
    "    seed=51, \n",
    "    )\n",
    "\n",
    "dataset_sample = dataset_sample.clone()\n",
    "\n",
    "dataset_sample.name = \"instruments_sample\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code overrides the default operator timeout. This is done because it may take a long time to generate embeddings, as they are coming from the Replicate API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.config.operator_timeout = 6_000_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The audio-retrieval-plugin allows you to search a dataset for images that are similar to a given audio clip. It works by using the `ImageBind` embedding model to embed images and audio clips into a shared \n",
    "\n",
    "To use the plugin, you need to:\n",
    "\n",
    "1. Start a local Qdrant server using Docker: `docker run -p \"6333:6333\" -p \"6334:6334\" -d qdrant/qdrant`\n",
    "\n",
    "2. Run the `create_imagebind_index` operator to create the similarity index\n",
    "\n",
    "3. Run the `open_audio_retrieval_panel` operator to open the search panel\n",
    "\n",
    "The search panel allows you to upload an audio clip (in `ogg` or `wav` format), and then searches the index for similar images. This plugin is a proof of concept and not intended for production use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
