{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/jacobmarks/reverse-image-search-plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "import fiftyone.zoo as foz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FiftyOne has integrations with Hugging Face, which allow you to easily pull datasets from the hub! Learn more about the integration [here](https://docs.voxel51.com/integrations/huggingface.html) and how you can pull datasets from the hub [here](https://docs.voxel51.com/integrations/huggingface.html#loading-datasets-from-the-hub)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.utils.huggingface as fouh\n",
    "\n",
    "stanford_cars_dataset = fouh.load_from_hub(\n",
    "    \"Multimodal-Fatima/StanfordCars_train\",\n",
    "    split=\"train\",\n",
    "    format= \"ParquetFilesDataset\",\n",
    "    max_samples=1000,\n",
    "    overwrite=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, you'll use a version of the [Stanford Cars dataset](https://ai.stanford.edu/~jkrause/papers/fgvc13.pdf) that a Hugging Face community member uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanford_cars_dataset = fo.load_dataset(\"Multimodal-Fatima/StanfordCars_train-1000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are just some fields that are unnecessary for our example. So, I'm just going to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanford_cars_dataset.delete_sample_fields(\n",
    "    [\n",
    "        \"clip_tags_ViT_L_14\",\n",
    "        \"LLM_Description_gpt3_downstream_tasks_ViT_L_14\",\n",
    "        \"LLM_Description_gpt3_downstream_tasks_visual_genome_ViT_L_14\",\n",
    "        \"blip_caption_beam_5\",\n",
    "        \"Attributes_ViT_L_14_text_davinci_003_full\",\n",
    "        \"Attributes_ViT_L_14_text_davinci_003_stanfordcars\",\n",
    "        \"clip_tags_ViT_L_14_with_openai_classes\",\n",
    "        \"clip_tags_ViT_L_14_wo_openai_classes\",\n",
    "        \"clip_tags_ViT_L_14_simple_specific\",\n",
    "        \"clip_tags_ViT_L_14_ensemble_specific\",\n",
    "        \"clip_tags_ViT_B_16_simple_specific\",\n",
    "        \"clip_tags_ViT_B_32_ensemble_specific\",\n",
    "        \"Attributes_ViT_B_16_descriptors_text_davinci_003_full\",\n",
    "        \"Attributes_LAION_ViT_H_14_2B_descriptors_text_davinci_003_full\",\n",
    "        \"clip_tags_LAION_ViT_H_14_2B_simple_specific\",\n",
    "        \"clip_tags_LAION_ViT_H_14_2B_ensemble_specific\",\n",
    "        \"Attributes_ViT_L_14_descriptors_text_davinci_003_full\",\n",
    "        \"clip_tags_ViT_B_16_ensemble_specific\"\n",
    "        ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ü¶í FiftyOne Model Zoo**\n",
    "\n",
    "The [FiftyOne Model Zoo ü¶í](https://docs.voxel51.com/user_guide/model_zoo/models.html) is a collection of over 70 pre-trained models that can be easily downloaded and run on FiftyOne Datasets. \n",
    "\n",
    "üìÇ This convenient resource provides a consistent interface for a wide variety of models, making it simple to integrate pre-trained models into your workflow.\n",
    "\n",
    "**üíª Basic Workflow**\n",
    "\n",
    "To get started, follow these steps:\n",
    "\n",
    "üîì **Step 1: Load a Model**: Load a model from the zoo using `foz.load_zoo_model()`.\n",
    "\n",
    "üìÅ **Step 2: Load a Dataset**: Load a dataset (or subset of one) to run the model on.\n",
    "\n",
    "üí° **Step 3: Generate Predictions**: Use methods like `apply_model()` to generate predictions, which are stored in the dataset.\n",
    "\n",
    "**üîç Advanced Features**\n",
    "\n",
    "-  **Embeddings**: Many zoo models can generate embeddings for samples or patches using the `compute_embeddings()` method.\n",
    "-  **Prediction Logits**: Many zoo models can optionally store prediction logits by passing `store_logits=True` to `apply_model()`. This enables running Brain methods like `compute_uniqueness()` and `compute_hardness()`.\n",
    "-  **Custom Models**: Custom models, such as PyTorch models, can be wrapped to implement the `Model` interface and used with builtin methods like `apply_model()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model = foz.load_zoo_model(\n",
    "    name=\"clip-vit-base32-torch\",\n",
    "    install_requirements=True,\n",
    ")\n",
    "\n",
    "shufflenet_model = foz.load_zoo_model(\n",
    "    name=\"shufflenetv2-0.5x-imagenet-torch\",\n",
    "    install_requirements=True,\n",
    ")\n",
    "\n",
    "resnet_model = foz.load_zoo_model(\n",
    "    name=\"resnet50-imagenet-torch\",\n",
    "    install_requirements=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[FiftyOne Brain](https://docs.voxel51.com/user_guide/brain.html) can generate embeddings and create indexes for images and objects or patches within images, which can be used for visualizations and indexes. It is compatible with various embedding models, dimensionality reduction techniques, and similarity backends.\n",
    "    \n",
    "üß† With the Brain you can:\n",
    "\n",
    "- üëÅÔ∏è Visualizing your dataset in a low-dimensional embedding space to reveal patterns and clusters\n",
    "\n",
    "- üóÇÔ∏è Indexing your data by similarity to easily find similar examples\n",
    "\n",
    "- ü¶Ñ Computing uniqueness measures for images to identify the most valuable unlabeled data to annotate\n",
    "\n",
    "- ‚ö†Ô∏è Identifying possible label mistakes in your annotations\n",
    "\n",
    "- üí° Finding the hardest samples for your model to learn from\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Brain runs are tracked and can be listed, loaded, renamed and deleted via the `Dataset` methods like `list_brain_runs()`, `load_brain_results()`, `rename_brain_run()`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanford_cars_dataset.compute_embeddings(\n",
    "    model=clip_model,\n",
    "    embeddings_field=\"clip_embeddings\",\n",
    "    progress=True,  \n",
    ")\n",
    "\n",
    "stanford_cars_dataset.compute_embeddings(\n",
    "    model=shufflenet_model,\n",
    "    embeddings_field=\"shufflenet_embeddings\",\n",
    "    progress=True,  \n",
    ")\n",
    "\n",
    "stanford_cars_dataset.compute_embeddings(\n",
    "    model=resnet_model,\n",
    "    embeddings_field=\"resnet_embeddings\",\n",
    "    progress=True,  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **üìä Compute Visualization**\n",
    "\n",
    "The `compute_visualization()` method üìä generates interactive visualizations of your dataset or patches in a low-dimensional space using UMAP, t-SNE, and PCA dimensionality reduction techniques.\n",
    "\n",
    "Visualizing datasets in low-dimensional embedding spaces helps reveal:\n",
    "\n",
    "üîπ Patterns and clusters that can help identify failure modes\n",
    "üîπ Similar examples and outliers\n",
    "üîπ New samples to add to your training set, helping you improve model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fob.compute_visualization(\n",
    "    stanford_cars_dataset,\n",
    "    embeddings=\"clip_embeddings\",\n",
    "    method=\"umap\",\n",
    "    brain_key = \"umap_2d_clip\",\n",
    "    num_dims=2,\n",
    "    num_workers = os.cpu_count(),\n",
    "    progress=True, \n",
    ")\n",
    "\n",
    "fob.compute_visualization(\n",
    "    stanford_cars_dataset,\n",
    "    embeddings=\"shufflenet_embeddings\",\n",
    "    method=\"umap\",\n",
    "    brain_key = \"umap_2d_shufflenet\",\n",
    "    num_dims=2,\n",
    "    num_workers = os.cpu_count(),\n",
    "    progress=True, \n",
    ")\n",
    "\n",
    "fob.compute_visualization(\n",
    "    stanford_cars_dataset,\n",
    "    embeddings=\"resnet_embeddings\",\n",
    "    method=\"umap\",\n",
    "    brain_key = \"umap_2d_resnet\",\n",
    "    num_dims=2,\n",
    "    num_workers = os.cpu_count(),\n",
    "    progress=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ü¶Ñ Compute Uniqueness**\n",
    "\n",
    "The `compute_uniqueness()` method üìä computes a uniqueness score for each image, comparing its content to all other images in the dataset.\n",
    "\n",
    "\n",
    "Computing uniqueness scores for images helps you:\n",
    "\n",
    "‚ú® Identify the most valuable data to annotate in the early stages of a machine learning workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fob.compute_uniqueness(\n",
    "    stanford_cars_dataset,\n",
    "    embeddings = \"clip_embeddings\",\n",
    "    uniqueness_field=\"car_uniqueness\",\n",
    "    num_workers = os.cpu_count(),\n",
    "    progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **üîç Compute Similarity**\n",
    "\n",
    "The `compute_similarity()` method üîç indexes your data by similarity, allowing you to efficiently search for similar samples or objects both programmatically and via point-and-click in the App.\n",
    "\n",
    "Choose from multiple backends to power your similarity index, including:\n",
    "\n",
    "üîπ Qdrant\n",
    "üîπ Redis\n",
    "üîπ Pinecone\n",
    "...and more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_index = fob.compute_similarity(\n",
    "    stanford_cars_dataset,\n",
    "    model=\"clip-vit-base32-torch\",\n",
    "    embeddings = \"clip_embeddings\",\n",
    "    brain_key=\"clip_sim_index\",\n",
    "    backend=\"lancedb\",\n",
    "    num_workers = os.cpu_count(),\n",
    "    metric=\"cosine\",\n",
    "    progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanford_cars_dataset.persistent = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(stanford_cars_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
