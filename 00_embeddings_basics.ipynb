{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll make use of the reverse image search plug in this notebook. You can install it like so: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/jacobmarks/reverse-image-search-plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "import fiftyone.zoo as foz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, you'll use a version of the [Stanford Cars dataset](https://ai.stanford.edu/~jkrause/papers/fgvc13.pdf) that a Hugging Face community member uploaded. \n",
    "\n",
    "FiftyOne has integrations with Hugging Face, which allow you to easily pull datasets from the hub! Learn more about the integration [here](https://docs.voxel51.com/integrations/huggingface.html) and how you can pull datasets from the hub [here](https://docs.voxel51.com/integrations/huggingface.html#loading-datasets-from-the-hub)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.utils.huggingface as fouh\n",
    "\n",
    "stanford_cars_dataset = fouh.load_from_hub(\n",
    "    \"Multimodal-Fatima/StanfordCars_train\",\n",
    "    split=\"train\",\n",
    "    format= \"ParquetFilesDataset\",\n",
    "    max_samples=2551,\n",
    "    name=\"stanford-cars\",\n",
    "    persist=True,\n",
    "    overwrite=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanford_cars_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the dataset above is persisted to disk (via the `persist=True` argument). The next time you load the dataset, all you have to do is run the following:\n",
    "\n",
    "```python\n",
    "\n",
    "stanford_cars_dataset = fo.load_dataset(\"stanford-cars\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are just some fields that are unnecessary for our example. So, I'm just going to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanford_cars_dataset.delete_sample_fields(\n",
    "    [\n",
    "        \"clip_tags_ViT_L_14\",\n",
    "        \"LLM_Description_gpt3_downstream_tasks_ViT_L_14\",\n",
    "        \"LLM_Description_gpt3_downstream_tasks_visual_genome_ViT_L_14\",\n",
    "        \"blip_caption_beam_5\",\n",
    "        \"Attributes_ViT_L_14_text_davinci_003_full\",\n",
    "        \"Attributes_ViT_L_14_text_davinci_003_stanfordcars\",\n",
    "        \"clip_tags_ViT_L_14_with_openai_classes\",\n",
    "        \"clip_tags_ViT_L_14_wo_openai_classes\",\n",
    "        \"clip_tags_ViT_L_14_simple_specific\",\n",
    "        \"clip_tags_ViT_L_14_ensemble_specific\",\n",
    "        \"clip_tags_ViT_B_16_simple_specific\",\n",
    "        \"clip_tags_ViT_B_32_ensemble_specific\",\n",
    "        \"Attributes_ViT_B_16_descriptors_text_davinci_003_full\",\n",
    "        \"Attributes_LAION_ViT_H_14_2B_descriptors_text_davinci_003_full\",\n",
    "        \"clip_tags_LAION_ViT_H_14_2B_simple_specific\",\n",
    "        \"clip_tags_LAION_ViT_H_14_2B_ensemble_specific\",\n",
    "        \"Attributes_ViT_L_14_descriptors_text_davinci_003_full\",\n",
    "        \"clip_tags_ViT_B_16_ensemble_specific\"\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ü¶í FiftyOne Model Zoo**\n",
    "\n",
    "The [FiftyOne Model Zoo ü¶í](https://docs.voxel51.com/user_guide/model_zoo/models.html) is a collection of pre-trained models that can be easily downloaded and run on FiftyOne Datasets. \n",
    "\n",
    "üìÇ It provides a convenient and consistent interface for a wide variety of models, making it simple to integrate pre-trained models into your workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model = foz.load_zoo_model(\n",
    "    name=\"clip-vit-base32-torch\",\n",
    "    install_requirements=True,\n",
    ")\n",
    "\n",
    "mobilenet_model = foz.load_zoo_model(\n",
    "    name=\"mobilenet-v2-imagenet-torch\",\n",
    "    install_requirements=True,\n",
    ")\n",
    "\n",
    "densenet_model = foz.load_zoo_model(\n",
    "    name=\"densenet121-imagenet-torch\",\n",
    "    install_requirements=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[FiftyOne Brain](https://docs.voxel51.com/user_guide/brain.html) can generate embeddings and create indexes for images and objects or patches within images, which can be used for visualizations and indexes. It is compatible with various embedding models, dimensionality reduction techniques, and similarity backends.\n",
    "    \n",
    "üß† With the Brain you can:\n",
    "\n",
    "- üëÅÔ∏è Visualizing your dataset in a low-dimensional embedding space to reveal patterns and clusters\n",
    "\n",
    "- üóÇÔ∏è Indexing your data by similarity to easily find similar examples\n",
    "\n",
    "- ü¶Ñ Computing uniqueness measures for images to identify the most valuable unlabeled data to annotate\n",
    "\n",
    "- ‚ö†Ô∏è Identifying possible label mistakes in your annotations\n",
    "\n",
    "- üí° Finding the hardest samples for your model to learn from\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Brain runs are tracked and can be listed, loaded, renamed and deleted via the `Dataset` methods like `list_brain_runs()`, `load_brain_results()`, `rename_brain_run()`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanford_cars_dataset.compute_embeddings(\n",
    "    model=clip_model,\n",
    "    embeddings_field=\"clip_embeddings\",\n",
    "    progress=True,  \n",
    ")\n",
    "\n",
    "stanford_cars_dataset.compute_embeddings(\n",
    "    model=mobilenet_model,\n",
    "    embeddings_field=\"mobilenet_embeddings\",\n",
    "    progress=True,\n",
    ")\n",
    "\n",
    "stanford_cars_dataset.compute_embeddings(\n",
    "    model=densenet_model,\n",
    "    embeddings_field=\"densenet_embeddings\",\n",
    "    progress=True,  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **üìä Compute Visualization**\n",
    "\n",
    "The `compute_visualization()` method üìä generates interactive visualizations of your dataset or patches in a low-dimensional space using UMAP, t-SNE, and PCA dimensionality reduction techniques.\n",
    "\n",
    "Visualizing datasets in low-dimensional embedding spaces helps reveal:\n",
    "\n",
    "üîπ Patterns and clusters that can help identify failure modes\n",
    "üîπ Similar examples and outliers\n",
    "üîπ New samples to add to your training set, helping you improve model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "fob.compute_visualization(\n",
    "    stanford_cars_dataset,\n",
    "    embeddings=\"clip_embeddings\",\n",
    "    method=\"umap\",\n",
    "    brain_key = \"umap_2d_clip\",\n",
    "    num_dims=2,\n",
    "    num_workers = os.cpu_count(),\n",
    "    progress=True, \n",
    ")\n",
    "\n",
    "fob.compute_visualization(\n",
    "    stanford_cars_dataset,\n",
    "    embeddings=\"densenet_embeddings\",\n",
    "    method=\"umap\",\n",
    "    brain_key = \"umap_2d_densenet\",\n",
    "    num_dims=2,\n",
    "    num_workers = os.cpu_count(),\n",
    "    progress=True, \n",
    ")\n",
    "\n",
    "fob.compute_visualization(\n",
    "    stanford_cars_dataset,\n",
    "    embeddings=\"mobilenet_embeddings\",\n",
    "    method=\"umap\",\n",
    "    brain_key = \"umap_2d_mobilenet\",\n",
    "    num_dims=2,\n",
    "    num_workers = os.cpu_count(),\n",
    "    progress=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ü¶Ñ Compute Uniqueness**\n",
    "\n",
    "The `compute_uniqueness()` method üìä computes a uniqueness score for each image, comparing its content to all other images in the dataset. In FiftyOne, uniqueness is calculated using a classification model-based approach:\n",
    "\n",
    "1. **Embedding**: Each sample is embedded into a vector space using the model.\n",
    "\n",
    "2. **k-Nearest Neighbors (kNN)**: For each sample, find its k nearest neighbors in the embedding space.\n",
    "\n",
    "3. **Distance-based Scoring**: Uniqueness is proportional to the distances from a sample to its kNNs.\n",
    "\n",
    "4. **Intuition**: Samples far from others in the embedding space are considered more unique.\n",
    "\n",
    "Key points:\n",
    "- `k` is a parameter of the uniqueness function\n",
    "- This method prioritizes outliers rather than cluster representatives\n",
    "- Contrasts with \"representativeness,\" which would emphasize samples central to dense clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "fob.compute_uniqueness(\n",
    "    stanford_cars_dataset,\n",
    "    embeddings = \"clip_embeddings\",\n",
    "    uniqueness_field=\"clip_uniqueness\",\n",
    "    num_workers = os.cpu_count(),\n",
    "    progress=True,\n",
    ")\n",
    "\n",
    "fob.compute_uniqueness(\n",
    "    stanford_cars_dataset,\n",
    "    embeddings = \"mobilenet_embeddings\",\n",
    "    uniqueness_field=\"mobilenet_uniqueness\",\n",
    "    num_workers = os.cpu_count(),\n",
    "    progress=True,\n",
    ")\n",
    "\n",
    "fob.compute_uniqueness(\n",
    "    stanford_cars_dataset,\n",
    "    embeddings = \"densenet_embeddings\",\n",
    "    uniqueness_field=\"densenet_uniqueness\",\n",
    "    num_workers = os.cpu_count(),\n",
    "    progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **üîç Compute Similarity**\n",
    "\n",
    "The `compute_similarity()` method üîç indexes your data by similarity, allowing you to efficiently search for similar samples or objects both programmatically and via point-and-click in the App.\n",
    "\n",
    "Choose from multiple backends to power your similarity index, including:\n",
    "\n",
    "üîπ LanceDB\n",
    "\n",
    "üîπ Milvus\n",
    "\n",
    "üîπ MongoDB\n",
    "\n",
    "...[and more](https://docs.voxel51.com/user_guide/brain.html#similarity-backends)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding model parameter here because the similarity index needs a model attached to it\n",
    "# in order to be used for search. for qdrant run the following: docker run -p \"6333:6333\" -p \"6334:6334\" -d qdrant/qdrant\n",
    "import fiftyone.brain as fob\n",
    "\n",
    "clip_sim_index = fob.compute_similarity(\n",
    "    stanford_cars_dataset,\n",
    "    model=\"clip-vit-base32-torch\",\n",
    "    brain_key=\"clip_sim_index\",\n",
    "    embeddings = \"clip_embeddings\",\n",
    "    backend=\"qdrant\",\n",
    "    metric=\"cosine\",\n",
    "    progress=True,\n",
    ")\n",
    "\n",
    "mobilenet_sim_index = fob.compute_similarity(\n",
    "    stanford_cars_dataset,\n",
    "    model=\"mobilenet-v2-imagenet-torch\",\n",
    "    brain_key=\"mbnet_sim_index\",\n",
    "    embeddings = \"mobilenet_embeddings\",\n",
    "    backend=\"qdrant\",\n",
    "    metric=\"cosine\",\n",
    "    progress=True,\n",
    ")\n",
    "\n",
    "densenet_sim_index = fob.compute_similarity(\n",
    "    stanford_cars_dataset,\n",
    "    model=\"densenet121-imagenet-torch\",\n",
    "    embeddings = \"densenet_embeddings\",\n",
    "    brain_key=\"densenet_sim_index\",\n",
    "    backend=\"qdrant\",\n",
    "    metric=\"cosine\",\n",
    "    progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representativeness\n",
    "\n",
    "We can use the [`compute_representativeness`](https://docs.voxel51.com/api/fiftyone.brain.html#fiftyone.brain.compute_representativeness) method from the FiftyOne Brain to find representative samples in our dataset.\n",
    "\n",
    "The core idea is that samples closer to cluster centers are considered more representative of the dataset, while the optional downweighting step helps ensure a diverse selection of representative samples.\n",
    "\n",
    "1. **Embedding Generation**:\n",
    "   - Samples are embedded into a vector space using a specified model or pre-computed embeddings.\n",
    "   - For ROI-based tasks, embeddings of multiple regions are aggregated (e.g., by taking the mean).\n",
    "\n",
    "2. **Clustering**:\n",
    "   - The algorithm uses either K-means (default) or MeanShift clustering on the embeddings.\n",
    "   - For K-means, the number of clusters (N) is set to 20 by default.\n",
    "\n",
    "3. **Distance Calculation**:\n",
    "   - For each sample, the distance to its nearest cluster center is computed.\n",
    "\n",
    "4. **Representativeness Scoring**:\n",
    "   - The initial representativeness score is calculated as: 1 / (1 + distance_to_cluster_center)\n",
    "   - This gives higher scores to samples closer to cluster centers.\n",
    "\n",
    "5. **Normalization**:\n",
    "   - Scores are normalized, either globally or per-cluster (default is per-cluster or \"local\" normalization).\n",
    "\n",
    "6. **Optional Redundancy Reduction**:\n",
    "   - If the \"cluster-center-downweight\" method is used, an additional step reduces redundancy.\n",
    "   - It iteratively downweights similar samples within a specified radius to promote diversity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "fob.compute_representativeness(\n",
    "    stanford_cars_dataset,\n",
    "    embeddings = \"clip_embeddings\",\n",
    ")\n",
    "\n",
    "fob.compute_representativeness(\n",
    "    stanford_cars_dataset,\n",
    "    embeddings = \"mobilenet_embeddings\",\n",
    ")\n",
    "\n",
    "fob.compute_representativeness(\n",
    "    stanford_cars_dataset,\n",
    "    embeddings = \"densenet_embeddings\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(stanford_cars_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
