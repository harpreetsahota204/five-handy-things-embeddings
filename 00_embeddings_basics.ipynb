{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll make use of the reverse image search plug in this notebook. You can install it like so: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/jacobmarks/reverse-image-search-plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "import fiftyone.zoo as foz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, you'll use a version of the [Stanford Cars dataset](https://ai.stanford.edu/~jkrause/papers/fgvc13.pdf) that a Hugging Face community member uploaded. \n",
    "\n",
    "FiftyOne has integrations with Hugging Face, which allow you to easily pull datasets from the hub! Learn more about the integration [here](https://docs.voxel51.com/integrations/huggingface.html) and how you can pull datasets from the hub [here](https://docs.voxel51.com/integrations/huggingface.html#loading-datasets-from-the-hub)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.utils.huggingface as fouh\n",
    "\n",
    "stanford_cars_dataset = fouh.load_from_hub(\n",
    "    \"Multimodal-Fatima/StanfordCars_train\",\n",
    "    split=\"train\",\n",
    "    format= \"ParquetFilesDataset\",\n",
    "    max_samples=1000,\n",
    "    name=\"stanford-cars\",\n",
    "    persist=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the dataset above is persisted to disk (via the `persist=True` argument). The next time you load the dataset, all you have to do is run the following:\n",
    "\n",
    "```python\n",
    "\n",
    "stanford_cars_dataset = fo.load_dataset(\"stanford-cars\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are just some fields that are unnecessary for our example. So, I'm just going to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanford_cars_dataset.delete_sample_fields(\n",
    "    [\n",
    "        \"clip_tags_ViT_L_14\",\n",
    "        \"LLM_Description_gpt3_downstream_tasks_ViT_L_14\",\n",
    "        \"LLM_Description_gpt3_downstream_tasks_visual_genome_ViT_L_14\",\n",
    "        \"blip_caption_beam_5\",\n",
    "        \"Attributes_ViT_L_14_text_davinci_003_full\",\n",
    "        \"Attributes_ViT_L_14_text_davinci_003_stanfordcars\",\n",
    "        \"clip_tags_ViT_L_14_with_openai_classes\",\n",
    "        \"clip_tags_ViT_L_14_wo_openai_classes\",\n",
    "        \"clip_tags_ViT_L_14_simple_specific\",\n",
    "        \"clip_tags_ViT_L_14_ensemble_specific\",\n",
    "        \"clip_tags_ViT_B_16_simple_specific\",\n",
    "        \"clip_tags_ViT_B_32_ensemble_specific\",\n",
    "        \"Attributes_ViT_B_16_descriptors_text_davinci_003_full\",\n",
    "        \"Attributes_LAION_ViT_H_14_2B_descriptors_text_davinci_003_full\",\n",
    "        \"clip_tags_LAION_ViT_H_14_2B_simple_specific\",\n",
    "        \"clip_tags_LAION_ViT_H_14_2B_ensemble_specific\",\n",
    "        \"Attributes_ViT_L_14_descriptors_text_davinci_003_full\",\n",
    "        \"clip_tags_ViT_B_16_ensemble_specific\"\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ü¶í FiftyOne Model Zoo**\n",
    "\n",
    "The [FiftyOne Model Zoo ü¶í](https://docs.voxel51.com/user_guide/model_zoo/models.html) is a collection of pre-trained models that can be easily downloaded and run on FiftyOne Datasets. \n",
    "\n",
    "üìÇ It provides a convenient and consistent interface for a wide variety of models, making it simple to integrate pre-trained models into your workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model = foz.load_zoo_model(\n",
    "    name=\"clip-vit-base32-torch\",\n",
    "    install_requirements=True,\n",
    ")\n",
    "\n",
    "dino_model = foz.load_zoo_model(\n",
    "    name=\"dinov2-vits14-torch\",\n",
    "    install_requirements=True,\n",
    ")\n",
    "\n",
    "densenet_model = foz.load_zoo_model(\n",
    "    name=\"densenet121-imagenet-torch\",\n",
    "    install_requirements=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[FiftyOne Brain](https://docs.voxel51.com/user_guide/brain.html) can generate embeddings and create indexes for images and objects or patches within images, which can be used for visualizations and indexes. It is compatible with various embedding models, dimensionality reduction techniques, and similarity backends.\n",
    "    \n",
    "üß† With the Brain you can:\n",
    "\n",
    "- üëÅÔ∏è Visualizing your dataset in a low-dimensional embedding space to reveal patterns and clusters\n",
    "\n",
    "- üóÇÔ∏è Indexing your data by similarity to easily find similar examples\n",
    "\n",
    "- ü¶Ñ Computing uniqueness measures for images to identify the most valuable unlabeled data to annotate\n",
    "\n",
    "- ‚ö†Ô∏è Identifying possible label mistakes in your annotations\n",
    "\n",
    "- üí° Finding the hardest samples for your model to learn from\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Brain runs are tracked and can be listed, loaded, renamed and deleted via the `Dataset` methods like `list_brain_runs()`, `load_brain_results()`, `rename_brain_run()`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanford_cars_dataset.compute_embeddings(\n",
    "    model=clip_model,\n",
    "    embeddings_field=\"clip_embeddings\",\n",
    "    progress=True,  \n",
    ")\n",
    "\n",
    "# DINOv2 is slow to run on CPU...feel free to run any other model from the zoo\n",
    "stanford_cars_dataset.compute_embeddings(\n",
    "    model=dino_model,\n",
    "    embeddings_field=\"dino_embeddings\",\n",
    "    progress=True,  \n",
    ")\n",
    "\n",
    "stanford_cars_dataset.compute_embeddings(\n",
    "    model=densenet_model,\n",
    "    embeddings_field=\"densenet_embeddings\",\n",
    "    progress=True,  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **üìä Compute Visualization**\n",
    "\n",
    "The `compute_visualization()` method üìä generates interactive visualizations of your dataset or patches in a low-dimensional space using UMAP, t-SNE, and PCA dimensionality reduction techniques.\n",
    "\n",
    "Visualizing datasets in low-dimensional embedding spaces helps reveal:\n",
    "\n",
    "üîπ Patterns and clusters that can help identify failure modes\n",
    "üîπ Similar examples and outliers\n",
    "üîπ New samples to add to your training set, helping you improve model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fob.compute_visualization(\n",
    "    stanford_cars_dataset,\n",
    "    embeddings=\"clip_embeddings\",\n",
    "    method=\"umap\",\n",
    "    brain_key = \"umap_2d_clip\",\n",
    "    num_dims=2,\n",
    "    num_workers = os.cpu_count(),\n",
    "    progress=True, \n",
    ")\n",
    "\n",
    "fob.compute_visualization(\n",
    "    stanford_cars_dataset,\n",
    "    embeddings=\"densenet_embeddings\",\n",
    "    method=\"umap\",\n",
    "    brain_key = \"umap_2d_densenet\",\n",
    "    num_dims=2,\n",
    "    num_workers = os.cpu_count(),\n",
    "    progress=True, \n",
    ")\n",
    "\n",
    "fob.compute_visualization(\n",
    "    stanford_cars_dataset,\n",
    "    embeddings=\"dino_embeddings\",\n",
    "    method=\"umap\",\n",
    "    brain_key = \"umap_2d_dino\",\n",
    "    num_dims=2,\n",
    "    num_workers = os.cpu_count(),\n",
    "    progress=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ü¶Ñ Compute Uniqueness**\n",
    "\n",
    "The `compute_uniqueness()` method üìä computes a uniqueness score for each image, comparing its content to all other images in the dataset.\n",
    "\n",
    "\n",
    "Computing uniqueness scores for images helps you identify the most valuable data to annotate in the early stages of a machine learning workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fob.compute_uniqueness(\n",
    "    stanford_cars_dataset,\n",
    "    embeddings = \"clip_embeddings\",\n",
    "    uniqueness_field=\"clip_uniqueness\",\n",
    "    num_workers = os.cpu_count(),\n",
    "    progress=True,\n",
    ")\n",
    "\n",
    "fob.compute_uniqueness(\n",
    "    stanford_cars_dataset,\n",
    "    embeddings = \"dino_embeddings\",\n",
    "    uniqueness_field=\"dino_uniqueness\",\n",
    "    num_workers = os.cpu_count(),\n",
    "    progress=True,\n",
    ")\n",
    "\n",
    "fob.compute_uniqueness(\n",
    "    stanford_cars_dataset,\n",
    "    embeddings = \"densenet_embeddings\",\n",
    "    uniqueness_field=\"densenet_uniqueness\",\n",
    "    num_workers = os.cpu_count(),\n",
    "    progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **üîç Compute Similarity**\n",
    "\n",
    "The `compute_similarity()` method üîç indexes your data by similarity, allowing you to efficiently search for similar samples or objects both programmatically and via point-and-click in the App.\n",
    "\n",
    "Choose from multiple backends to power your similarity index, including:\n",
    "\n",
    "üîπ LanceDB\n",
    "\n",
    "üîπ Milvus\n",
    "\n",
    "üîπ MongoDB\n",
    "\n",
    "...[and more](https://docs.voxel51.com/user_guide/brain.html#similarity-backends)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embeddings...\n",
      "   0% |---------------|    3/1000 [1.1s elapsed, 5.9m remaining, 2.8 samples/s]    "
     ]
    }
   ],
   "source": [
    "# adding model parameter here because the similarity index needs a model attached to it\n",
    "# in order to be used for search\n",
    "\n",
    "clip_sim_index = fob.compute_similarity(\n",
    "    stanford_cars_dataset,\n",
    "    model=\"clip-vit-base32-torch\",\n",
    "    brain_key=\"clip_sim_index\",\n",
    "    backend=\"qdrant\",\n",
    "    metric=\"cosine\",\n",
    "    progress=True,\n",
    ")\n",
    "\n",
    "dino_sim_index = fob.compute_similarity(\n",
    "    stanford_cars_dataset,\n",
    "    model=\"dinov2-vits14-torch\",\n",
    "    brain_key=\"dino_sim_index\",\n",
    "    backend=\"qdrant\",\n",
    "    metric=\"cosine\",\n",
    "    progress=True,\n",
    ")\n",
    "\n",
    "densenet_sim_index = fob.compute_similarity(\n",
    "    stanford_cars_dataset,\n",
    "    model=\"densenet121-imagenet-torch\",\n",
    "    brain_key=\"densenet_sim_index\",\n",
    "    backend=\"qdrant\",\n",
    "    metric=\"cosine\",\n",
    "    progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanford_cars_dataset.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(stanford_cars_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
