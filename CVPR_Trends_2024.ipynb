{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xJKweLg28gm"
      },
      "source": [
        "# üëãüèΩ Hey - I'm [Harpreet](https://twitter.com/DataScienceHarp), Hacker-in-Residence at [Voxel51](https://huggingface.co/Voxel51)!\n",
        "\n",
        "The 2024  Conference on Computer Vision and Pattern Recognition (CVPR) is happening in just under a week! I'll be there with my crew from [Voxel 51](https://voxel51.com/) at **Booth 1519**, which will be located right next to the Meta and Amazon Science booths! If you found this notebook useful, come by and say \"Hi\" and I'll hook you up with some swag!\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*CoYrezxPyB7zbqYYv94B8w.png\">\n",
        "\n",
        "\n",
        "# ü§î What's this notebook about?\n",
        "\n",
        "\n",
        "The 2024 Conference on Computer Vision and Pattern Recognition (CVPR) received 11,532 valid paper submissions, and only 2,719 were accepted for an overall acceptance rate of about 23.6%.\n",
        "\n",
        "But keeping up with the vast array of research being presented at this year's CVPR can be challenging. CVPR has an [awesome website](https://cvpr.thecvf.com/virtual/2024/papers.html) listing out all the paper, but the information I want is scattered across various links and platforms. Needless to say, getting a good idea of what's being presented is time-consuming (and a bit disorganized).\n",
        "\n",
        "But what if you could access all this knowledge in one convenient location, allowing you to easily identify trends and gain valuable insights?\n",
        "\n",
        "Well, I curated a dataset, hosted on Hugging Face and built with FiftyOne, that does just that ‚Äì help you explore this year's conference offerings. I was able to find/scrape 2,389 of the 2,719 accepted papers and I put them into a dataset that we're going to explore together!\n",
        "\n",
        "## üßê What's in this dataset?\n",
        "\n",
        "The dataset consists of images of first pages of papers, their titles, list of authors, their abstracts, direct links to papers on arXiv, project pages, a category breakdown according to the arXiv taxonomy, and keywords that I bucketed from the [2024 CVPR call for papers](https://cvpr.thecvf.com/Conferences/2024/CallForPapers).\n",
        "\n",
        "Here are the fields:\n",
        "\n",
        " - An image of the first page of the paper\n",
        "\n",
        " - `title`: The title of the paper\n",
        "\n",
        " - `authors_list`: The list of authors\n",
        "\n",
        " - `abstract`: The abstract of the paper\n",
        "\n",
        " - `arxiv_link`: Link to the paper on arXiv\n",
        "\n",
        " - `other_link`: Link to the project page, if found\n",
        "\n",
        " - `category_name`: The primary category this paper according to [arXiv taxonomy](https://arxiv.org/category_taxonomy)\n",
        "\n",
        " - `all_categories`: All categories this paper falls into, according to arXiv taxonomy\n",
        "\n",
        " - `keywords`: Extracted using GPT-4o\n",
        "\n",
        "This should give us enough information to pick up on some interesting trends for this years CVPR!\n",
        "\n",
        "\n",
        "PS: You can check out my picks for awesome papers at CVPR in [my GitHub repo](https://github.com/harpreetsahota204/awesome-cvpr-2024). Here's some general code for [how I scraped the CVPR data](https://github.com/harpreetsahota204/CVPR-2024-Papers).\n",
        "\n",
        "\n",
        "Let's start by installing some dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeC4fh5cWezQ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install fiftyone sentence-transformers umap-learn lancedb scikit-learn==1.4.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gveV2gQyGu0e"
      },
      "source": [
        "This tutorial will make use of the clustering plugin. Checkout all available plugins [here](github.com/voxel51/fiftyone-plugins)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sooFF5ZoinzK"
      },
      "outputs": [],
      "source": [
        "!fiftyone plugins download https://github.com/jacobmarks/clustering-plugin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbqJ0s8mMchA"
      },
      "outputs": [],
      "source": [
        "import fiftyone as fo\n",
        "import fiftyone.utils.huggingface as fouh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o1tyVxab56y"
      },
      "source": [
        "FiftyOne integrates natively with Hugging Face's Transformers library.\n",
        "\n",
        "You can easily load, fine-tune, and run inference with Transformers models on FiftyOne datasets. It also integrates with the Hugging Face Hub, which allows you to push datasets to and load datasets from the Hub. It's a nice integration that simplifies sharing datasets with the machine learning community and accessing popular vision datasets. You can load datasets from specific revisions, handle multiple media fields, and configure advanced settings through the integration - check out the Hugging Face organization page[here](https://huggingface.co/Voxel51) to see what datasets are available.\n",
        "\n",
        "\n",
        "I've [posted the dataset on Hugging Face](https://huggingface.co/datasets/Voxel51/CVPR_2024_Papers) - feel free to smash a like on it to help spread the word - and you can access it as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhGXfHzIsk6r"
      },
      "outputs": [],
      "source": [
        "dataset = fouh.load_from_hub(\"harpreetsahota/CVPR_2024_Papers_with_Embeddings\", persistent=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dataset = fo.load_dataset(\"CVPR_2024_Papers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GEQQDXb1Mba"
      },
      "source": [
        "You've now loaded the dataset into FiftyOne format!\n",
        "\n",
        "The FiftyOne dataset object is a core component of the FiftyOne library. It's the central hub for managing and interacting with datasets in the FiftyOne ecosystem. It gives you a high-level interface for performing various dataset-related tasks, such as loading data, applying transformations, evaluating models, and exporting datasets in different formats.\n",
        "\n",
        "The dataset object represents a collection of samples along with fields (associated metadata, labels, and other annotations). The dataset object provides a convenient way to store, manipulate, and query datasets in FiftyOne.\n",
        "\n",
        "Some key features of the FiftyOne dataset object include:\n",
        "\n",
        "1. **Support for various data types:** The dataset object can handle different types of data, such as images, videos, and associated annotations like bounding boxes, segmentation masks, arbitrary text, and classification labels.\n",
        "\n",
        "2. **Flexible metadata:** Each sample in the dataset can have associated metadata, which can be used to store additional information about the sample, such as its source, attributes, or custom fields.\n",
        "\n",
        "3. **Powerful querying:** FiftyOne provides a query language that allows you to filter and select subsets of samples based on their metadata, labels, or other criteria.\n",
        "\n",
        "4. **Visualization:** The dataset object integrates with FiftyOne's visualization tools, enabling you to visualize samples, annotations, and model predictions directly in the browser.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQen19wJuSXR"
      },
      "outputs": [],
      "source": [
        "session = fo.launch_app(dataset, auto=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gh_6keP9yFQC"
      },
      "source": [
        "Take a look at the app below.\n",
        "\n",
        "With it you can get insight into the distribution of keywords, categories, and the number of papers a given author (or, at least, someone with that name) has attributed to them at this years conference!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PLQDg-DFdJu"
      },
      "outputs": [],
      "source": [
        "session.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvnE9zQYyBKO"
      },
      "source": [
        "\n",
        "\n",
        "You can do more interesting analysis from here. Start by getting embeddings for the title and abstract of each paper. For that, you can make use of [`gte-large-en-v1.5`](Alibaba-NLP/gte-large-en-v1.5). It's small, it's fast, and it's good.\n",
        "\n",
        "Of course, feel free to choose any model you'd like.\n",
        "\n",
        "<img src=\"https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExMm12NGFhazJzOG4xYndpanh4amo2dmozNHBwM20yM3RsN283c2M4NCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/RoG1eqznuRskHxPsJe/giphy.gif\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMlAdI64cUFf"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer(\n",
        "    'Alibaba-NLP/gte-large-en-v1.5',\n",
        "    trust_remote_code=True,\n",
        "    # force_download=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUXXnrzR06uQ"
      },
      "source": [
        "The code below will helpgenerate and add text embeddings to a FiftyOne dataset.\n",
        "\n",
        "1. `get_text_embeddings(dataset, field, model)`:\n",
        "\n",
        "   - This function takes a FiftyOne dataset, a field name containing text data, and a pre-trained embedding model.\n",
        "\n",
        "   - It retrieves the text data from the specified field of the dataset.\n",
        "\n",
        "   - It generates embeddings for each text using the provided embedding model.\n",
        "\n",
        "   - It returns a list of embeddings.\n",
        "\n",
        "2. `add_embeddings_to_dataset(dataset, field, embeddings)`:\n",
        "\n",
        "   - This function takes a FiftyOne dataset, a field name to store the embeddings, and a list of embeddings.\n",
        "\n",
        "   - It adds a new sample field to the dataset to store the embeddings.\n",
        "\n",
        "   - It sets the values of the newly added field to the provided embeddings.\n",
        "\n",
        "Basically, it will:\n",
        "\n",
        "1. Extract text data from a specific field in a FiftyOne dataset.\n",
        "\n",
        "2. Generate embeddings for each text using a pre-trained embedding model.\n",
        "\n",
        "3. Add the generated embeddings back to the dataset as a new field.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSfK6KMUfAQW"
      },
      "outputs": [],
      "source": [
        "def get_text_embeddings(dataset, field, model):\n",
        "  \"\"\"\n",
        "  Returns the embeddings of the abstracts in the dataset.\n",
        "\n",
        "  Args:\n",
        "    dataset: A FiftyOne dataset object.\n",
        "\n",
        "  Returns:\n",
        "    A list of embeddings.\n",
        "  \"\"\"\n",
        "  texts = dataset.values(field)\n",
        "  text_embeddings = []\n",
        "  for text in texts:\n",
        "      embeddings = model.encode(text)\n",
        "      text_embeddings.append(embeddings)\n",
        "  return text_embeddings\n",
        "\n",
        "\n",
        "def add_embeddings_to_dataset(dataset, field, embeddings):\n",
        "  \"\"\"\n",
        "  Adds the embeddings to the dataset.\n",
        "\n",
        "  Args:\n",
        "    dataset: A FiftyOne dataset object.\n",
        "    embeddings: A list of embeddings.\n",
        "  \"\"\"\n",
        "  dataset.add_sample_field(field, fo.VectorField)\n",
        "  dataset.set_values(field, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44LkVD6oISyi"
      },
      "source": [
        "Now, add the embeddings to the dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hw68uPvFgZgW"
      },
      "outputs": [],
      "source": [
        "abstract_embeddings = get_text_embeddings(\n",
        "    dataset = dataset,\n",
        "    field = \"abstract\",\n",
        "    model = model\n",
        ")\n",
        "\n",
        "add_embeddings_to_dataset(\n",
        "    dataset=dataset,\n",
        "    field=\"abstract_embeddings\",\n",
        "    embeddings=abstract_embeddings\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSObxk6KgkxF"
      },
      "outputs": [],
      "source": [
        "title_embeddings = get_text_embeddings(\n",
        "    dataset = dataset,\n",
        "    field = \"title\",\n",
        "    model = model\n",
        ")\n",
        "\n",
        "add_embeddings_to_dataset(\n",
        "    dataset=dataset,\n",
        "    field=\"title_embeddings\",\n",
        "    embeddings=title_embeddings\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAV30GIZ3bHk"
      },
      "source": [
        "## Making use of the embeddings\n",
        "\n",
        "You can use [FiftyOne Brain](https://docs.voxel51.com/user_guide/brain.html) to do some cool stuff with embeddings, like:\n",
        "\n",
        "- Visualizing datasets in low-dimensional embedding spaces to reveal patterns and clusters that can help identify failure modes, critical scenarios, and recommend new samples to add to your training set\n",
        "\n",
        "- Computing uniqueness scores for images to identify the most unique samples that are vital for efficient model training in the early stages of the machine learning workflow\n",
        "\n",
        "- Indexing datasets by similarity to easily find similar examples to images or objects of interest, which is useful for diagnosing model issues or mining for additional training data\n",
        "\n",
        "\n",
        "Below are the supported dimensionality reduction methods in the Brain:\n",
        "\n",
        "### UMAP (Uniform Manifold Approximation and Projection)\n",
        "\n",
        "UMAP  is a dimensionality reduction technique that uses applied Riemannian geometry and algebraic topology to find low-dimensional embeddings of structured data.\n",
        "\n",
        "It is particularly well-suited for text embeddings because it can handle high-dimensional data and preserve the global structure of the data, making it useful for both visualization and preprocessing for clustering algorithms.\n",
        "\n",
        "### t-SNE (t-distributed Stochastic Neighbor Embedding)\n",
        "\n",
        "t-SNE is a non-linear dimensionality reduction technique that is also used for visualizing high-dimensional data. It is similar to UMAP but tends to be slower and less scalable.\n",
        "\n",
        "While it can be effective for certain types of data, it may not perform as well as UMAP for large datasets.\n",
        "\n",
        "### PCA (Principal Component Analysis)\n",
        "\n",
        "PCA is a linear dimensionality reduction technique that projects high-dimensional data onto lower-dimensional subspaces. It is fast and easy to implement but may not capture non-linear relationships in the data as effectively as UMAP or t-SNE.\n",
        "\n",
        "PCA is often used for simpler data sets where linearity is a reasonable assumption.\n",
        "\n",
        "### Manual\n",
        "\n",
        "Manually computing a low-dimensional representation involves creating a custom method to reduce the dimensionality of the data. This approach can be time-consuming and requires a deep understanding of the data and the desired outcome.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "il5U9Nx4iLXb"
      },
      "outputs": [],
      "source": [
        "import fiftyone.brain as fob\n",
        "\n",
        "fob.compute_visualization(\n",
        "    dataset,\n",
        "    embeddings=\"abstract_embeddings\",\n",
        "    num_dims=2,\n",
        "    method=\"umap\",\n",
        "    brain_key=\"umap_abstract\",\n",
        "    verbose=True,\n",
        "    seed=51\n",
        ")\n",
        "\n",
        "fob.compute_visualization(\n",
        "    dataset,\n",
        "    embeddings=\"title_embeddings\",\n",
        "    num_dims=2,\n",
        "    method=\"umap\",\n",
        "    brain_key=\"umap_title\",\n",
        "    verbose=True,\n",
        "    seed=51\n",
        ")\n",
        "\n",
        "\n",
        "fob.compute_visualization(\n",
        "    dataset,\n",
        "    embeddings=\"abstract_embeddings\",\n",
        "    num_dims=2,\n",
        "    method=\"tsne\",\n",
        "    brain_key=\"tsne_abstract\",\n",
        "    verbose=True,\n",
        "    seed=51\n",
        ")\n",
        "\n",
        "fob.compute_visualization(\n",
        "    dataset,\n",
        "    embeddings=\"title_embeddings\",\n",
        "    num_dims=2,\n",
        "    method=\"tsne\",\n",
        "    brain_key=\"tsne_title\",\n",
        "    verbose=True,\n",
        "    seed=51\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNQLtGXeBK_n"
      },
      "source": [
        "The code below adds a uniqueness field to each sample scoring how unique it is with respect to the rest of the samples. This is itersting because you can understand what are the most unique papers (based on their abstracts) among the all the papers in the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Judh1w_9XbD0"
      },
      "outputs": [],
      "source": [
        "fob.compute_uniqueness(\n",
        "    dataset,\n",
        "    embeddings=\"abstract_embeddings\",\n",
        "    uniqueness_field=\"uniqueness_abstract\",\n",
        ")\n",
        "\n",
        "fob.compute_uniqueness(\n",
        "    dataset,\n",
        "    embeddings=\"title_embeddings\",\n",
        "    uniqueness_field=\"uniqueness_title\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z5O9wvZEO1e"
      },
      "source": [
        "\n",
        "The code below will index the abstract embeddings by similarity and you can easily query and sort your datasets to find similar examples. Once you‚Äôve indexed a dataset by similarity, you can use the `sort_by_similarity()` view stage to programmatically sort the dataset by abstract similarity! The code below is using LanceDB as the back end(read about the integration [here](https://docs.voxel51.com/integrations/lancedb.html)) but there at the moment there several backends you can use:\n",
        "\n",
        "‚Ä¢ sklearn (default): a scikit-learn backend\n",
        "\n",
        "‚Ä¢ qdrant: a Qdrant backend\n",
        "\n",
        "‚Ä¢ redis: a Redis backend\n",
        "\n",
        "‚Ä¢ pinecone: a Pinecone backend\n",
        "\n",
        "‚Ä¢ mongodb: a MongoDB backend\n",
        "\n",
        "‚Ä¢ milvus: a Milvus backend\n",
        "\n",
        "The library is opensource and we welcome contributions, feel free to contribute an integration to your favorite vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJ_x4oAuiST0"
      },
      "outputs": [],
      "source": [
        "sim_abstract = fob.compute_similarity(\n",
        "    dataset,\n",
        "    embeddings=\"abstract_embeddings\",\n",
        "    brain_key=\"abstract_similarity\",\n",
        "    backend=\"lancedb\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clip_abstract = fob.compute_similarity(\n",
        "    dataset,\n",
        "    model=\"clip-vit-base32-torch\",\n",
        "    brain_key=\"clip_embeddings\",\n",
        "    backend=\"lancedb\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkaQnWwYGo0v"
      },
      "source": [
        "Now, let's check all this out in the app!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2P4V6fzriixN"
      },
      "outputs": [],
      "source": [
        "session = fo.launch_app(dataset, auto=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "session.url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "pTkWUTLNglkh",
        "outputId": "84da810e-c546-446f-aa13-89a07b2a9d01"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "html_code = \"\"\"\n",
        "<blockquote class=\"imgur-embed-pub\" lang=\"en\" data-id=\"a/m4WFXWX\" data-context=\"false\" ><a href=\"//imgur.com/a/m4WFXWX\"></a></blockquote><script async src=\"//s.imgur.com/min/embed.js\" charset=\"utf-8\"></script>\n",
        "\"\"\"\n",
        "\n",
        "display(HTML(html_code))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yz6Hdl5sNEPL"
      },
      "source": [
        "Check out [this short video](https://share.descript.com/view/smbmqypvtaN), where I'll show you how to explore what's happening at CVPR using the app!\n",
        "\n",
        "There's a lot more that you can do with FiftyOne, more than I can share in this note-blog. But, I hope you'll join me for a workshop where I'll spend ~90 minutes teaching you how to use FiftyOne! Sign up [here](https://voxel51.com/computer-vision-events/getting-started-with-fiftyone-workshop-june-26-2024/)!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
