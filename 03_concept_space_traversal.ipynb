{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept Space Traversal\n",
    "\n",
    "Concept space traversal refers to navigating and exploring the high-dimensional vector space in which data points (such as words, images, or other entities) are embedded based on their semantic relationships or similarities. You can use this for exploring and manipulating the semantic relationships captured by embeddings, enabling applications like concept generation, similarity search, and analogy discovery. \n",
    "\n",
    "It provides a way to navigate the abstract space of concepts and ideas represented in high-dimensional vector spaces.\n",
    "\n",
    "Key points about concept space traversal:\n",
    "\n",
    "1. **Embedding space:** Data points are represented as vectors in a high-dimensional space, where similar concepts are positioned close to each other based on their semantic relationships. This space is often referred to as the embedding space or concept space.\n",
    "\n",
    "2. **Similarity measures:** The proximity or similarity between data points in the embedding space is typically measured using cosine similarity or Euclidean distance. Points that are closer together are considered more semantically similar.\n",
    "\n",
    "3. **Traversal methods:** Concept space traversal involves moving from one point to another within the embedding space. This can be done through various methods, such as:\n",
    "\n",
    "   - Linear interpolation: Creating intermediate points between two concepts by taking weighted averages of their vector representations. \n",
    "\n",
    "   - Vector arithmetic: Performing operations like addition or subtraction on concept vectors to find analogies or explore relationships.\n",
    "\n",
    "   - Nearest neighbour search: Finding the closest points to a given concept vector to discover related concepts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "import fiftyone.zoo as foz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, you'll need to manually download the Colorswap dataset. You can find the dataset [here](https://github.com/Top34051/colorswap?tab=readme-ov-file).\n",
    "\n",
    "Once the dataset is downloaded, you can get it into FiftyOne format.\n",
    "\n",
    "First, you will need to \"unpack\" the json file that comes with the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "images_path = \"./colorswap\"\n",
    "\n",
    "# Load train.json\n",
    "with open('./colorswap/train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "# Load test.json\n",
    "with open('/Users/harpreetsahota/workspace/datasets/colorswap/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Combine the two datasets\n",
    "packed_annotations = train_data + test_data\n",
    "\n",
    "unpacked_annotations = []\n",
    "\n",
    "for item in packed_annotations:\n",
    "    unpacked_annotations.append({\n",
    "        \"image_path\": os.path.join(images_path, item[\"image_1\"]),\n",
    "        \"caption\": item[\"caption_1\"],\n",
    "        \"image_source\": item[\"image_source\"]\n",
    "    })\n",
    "    unpacked_annotations.append({\n",
    "        \"image_path\": os.path.join(images_path, item[\"image_2\"]),\n",
    "        \"caption\": item[\"caption_2\"],\n",
    "        \"image_source\": item[\"image_source\"]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our custom labels, we can write helper functions that will define the schema for the FiftyOne dataset object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.core.fields as fof\n",
    "import os\n",
    "\n",
    "def create_colorswap_dataset(name) -> fo.Dataset:\n",
    "\t\"\"\"\n",
    "\tCreates schema for a FiftyOne dataset.\n",
    "\t\"\"\"\n",
    "\tdataset = fo.Dataset(name=name, persistent=True, overwrite=True)\n",
    "\n",
    "\tdataset.add_sample_field(\n",
    "\t\t'prompt', \n",
    "\t\tfof.StringField,\n",
    "\t\tdescription='Prompt that generated image'\n",
    "\t\t)\n",
    "\n",
    "\tdataset.add_sample_field(\n",
    "\t\t'image_source', \n",
    "\t\tfof.StringField,\n",
    "\t\tdescription='Model that generated image'\n",
    "\t\t)\n",
    "\t\n",
    "\treturn dataset\n",
    "\n",
    "\n",
    "def create_fo_sample(image: dict) -> fo.Sample:\n",
    "    \"\"\"\n",
    "    Creates a FiftyOne Sample from a given image entry with metadata and custom fields.\n",
    "\n",
    "    Args:\n",
    "        image (dict): A dictionary containing image data including the path and other properties.\n",
    "\n",
    "    Returns:\n",
    "        fo.Sample: The FiftyOne Sample object with the image and its metadata.\n",
    "    \"\"\"\n",
    "    \n",
    "    filepath = image.get('image_path')\n",
    "    \n",
    "    if not filepath:\n",
    "        return None\n",
    "\n",
    "    prompt = image.get('caption')\n",
    "    image_source = image.get('image_source')\n",
    "\n",
    "    sample = fo.Sample(\n",
    "        filepath=filepath,\n",
    "        prompt=prompt,\n",
    "        image_source=image_source,\n",
    "    )\n",
    "\n",
    "    return sample\n",
    "\n",
    "def add_samples_to_fiftyone_dataset(\n",
    "\tdataset: fo.Dataset,\n",
    "\tsamples: list\n",
    "\t):\n",
    "\t\"\"\"\n",
    "\tCreates a FiftyOne dataset from a list of samples.\n",
    "\n",
    "\tArgs:\n",
    "\t\tsamples (list): _description_\n",
    "\t\tdataset_name (str): _description_\n",
    "\t\"\"\"\n",
    "\tdataset.add_samples(samples, dynamic=True)\n",
    "\tdataset.add_dynamic_sample_fields()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load it into FiftyOne format, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_colorswap_dataset(\"colorswap\")\n",
    "\n",
    "samples = [create_fo_sample(image) for image in unpacked_annotations]\n",
    "\n",
    "add_samples_to_fiftyone_dataset(dataset, samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Concept Traversal Plugin for FiftyOne allows users to navigate the space of concepts in their dataset using both text and images. \n",
    "\n",
    "Key points:\n",
    "\n",
    "- You select a starting image from their dataset, then iteratively add text concepts with relative strengths to move around the multimodal embedding space.\n",
    "\n",
    "- Behind the scenes, it generates embedding vectors for the text prompts, combines them with the starting image vector, and performs a similarity search on the dataset.\n",
    "\n",
    "- Creating the plugin required generating a multimodal similarity index (e.g. using a CLIP model) on the dataset first.\n",
    "\n",
    "To use the plugin, a similarity index that supports prompts (i.e., embeds both text and images) must be present on the dataset. \n",
    "\n",
    "This can be created using the `fiftyone.brain` module, specifically the `compute_similarity` function, which takes the dataset, a `brain_key`, the `model_name` (e.g., `clip-vit-base32-torch`), and the `metric` (e.g., `cosine`) as arguments.\n",
    "\n",
    "The plugin can be installed by running the command `fiftyone plugins download https://github.com/jacobmarks/concept-interpolation`.\n",
    "\n",
    "The plugin provides two main operators:\n",
    "\n",
    "1. `open_interpolation_panel`: Opens the interpolation panel when clicked, but is only activated when the dataset has a similarity index.\n",
    "\n",
    "2. `interpolator`: Runs the actual interpolation between the two text prompts.\n",
    "\n",
    "In summary, this FiftyOne plugin enables users to explore the latent space between two text concepts by interpolating between their embeddings and visualizing the results, providing an interactive way to understand the relationships between different text prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "color_swap = fo.load_dataset(\"colorswap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Brain method run with key 'concept_embeddings' already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_similarity\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolor_swap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbrain_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconcept_embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclip_embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclip-vit-base32-torch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcosine\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fiftyone/lib/python3.10/site-packages/fiftyone/brain/__init__.py:550\u001b[0m, in \u001b[0;36mcompute_similarity\u001b[0;34m(samples, patches_field, embeddings, brain_key, model, model_kwargs, force_square, alpha, batch_size, num_workers, skip_failures, progress, backend, **kwargs)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Uses embeddings to index the samples or their patches so that you can\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;124;03mquery/sort by similarity.\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;124;03m    a :class:`fiftyone.brain.similarity.SimilarityIndex`\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfiftyone\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbrain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimilarity\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mfbs\u001b[39;00m\n\u001b[0;32m--> 550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfbs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_similarity\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatches_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbrain_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_square\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_failures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fiftyone/lib/python3.10/site-packages/fiftyone/brain/similarity.py:107\u001b[0m, in \u001b[0;36mcompute_similarity\u001b[0;34m(samples, patches_field, embeddings, brain_key, model, model_kwargs, force_square, alpha, batch_size, num_workers, skip_failures, progress, backend, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m brain_method\u001b[38;5;241m.\u001b[39mensure_requirements()\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m brain_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# Don't allow overwriting an existing run with same key, since we\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# need the existing run in order to perform workflows like\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# automatically cleaning up the backend's index\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     \u001b[43mbrain_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbrain_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m results \u001b[38;5;241m=\u001b[39m brain_method\u001b[38;5;241m.\u001b[39minitialize(samples, brain_key)\n\u001b[1;32m    111\u001b[0m get_embeddings \u001b[38;5;241m=\u001b[39m embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/fiftyone/fiftyone/core/runs.py:313\u001b[0m, in \u001b[0;36mBaseRun.register_run\u001b[0;34m(self, samples, key, overwrite, cleanup)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 313\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m version \u001b[38;5;241m=\u001b[39m foc\u001b[38;5;241m.\u001b[39mVERSION\n\u001b[1;32m    315\u001b[0m timestamp \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mutcnow()\n",
      "File \u001b[0;32m~/workspace/fiftyone/fiftyone/core/runs.py:350\u001b[0m, in \u001b[0;36mBaseRun.validate_run\u001b[0;34m(self, samples, key, overwrite)\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m overwrite:\n\u001b[0;32m--> 350\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m with key \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m already exists\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_str()\u001b[38;5;241m.\u001b[39mcapitalize(), key)\n\u001b[1;32m    353\u001b[0m     )\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m     existing_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_run_info(samples, key)\n",
      "\u001b[0;31mValueError\u001b[0m: Brain method run with key 'concept_embeddings' already exists"
     ]
    }
   ],
   "source": [
    "fob.compute_similarity(\n",
    "    color_swap,\n",
    "    brain_key=\"concept_embeddings\",\n",
    "    embeddings=\"clip_embeddings\",\n",
    "    model=\"clip-vit-base32-torch\",\n",
    "    metric=\"cosine\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fob.compute_visualization(\n",
    "    color_swap,\n",
    "    embeddings=\"clip_embeddings\",\n",
    "    method=\"umap\",\n",
    "    brain_key = \"umap_2d_clip\",\n",
    "    num_dims=2,\n",
    "    num_workers = os.cpu_count(),\n",
    "    progress=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(color_swap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
