{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept Space Traversal\n",
    "\n",
    "Concept space traversal refers to navigating and exploring the high-dimensional vector space in which data points (such as words, images, or other entities) are embedded based on their semantic relationships or similarities. You can use this for exploring and manipulating the semantic relationships captured by embeddings, enabling applications like concept generation, similarity search, and analogy discovery. \n",
    "\n",
    "It provides a way to navigate the abstract space of concepts and ideas represented in high-dimensional vector spaces.\n",
    "\n",
    "Key points about concept space traversal:\n",
    "\n",
    "1. **Embedding space:** Data points are represented as vectors in a high-dimensional space, where similar concepts are positioned close to each other based on their semantic relationships. This space is often referred to as the embedding space or concept space.\n",
    "\n",
    "2. **Similarity measures:** The proximity or similarity between data points in the embedding space is typically measured using cosine similarity or Euclidean distance. Points that are closer together are considered more semantically similar.\n",
    "\n",
    "3. **Traversal methods:** Concept space traversal involves moving from one point to another within the embedding space. This can be done through various methods, such as:\n",
    "\n",
    "   - Linear interpolation: Creating intermediate points between two concepts by taking weighted averages of their vector representations. \n",
    "\n",
    "   - Vector arithmetic: Performing operations like addition or subtraction on concept vectors to find analogies or explore relationships.\n",
    "\n",
    "   - Nearest neighbour search: Finding the closest points to a given concept vector to discover related concepts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "import fiftyone.zoo as foz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, you'll need to manually download the Colorswap dataset. You can find the dataset [here](https://github.com/Top34051/colorswap?tab=readme-ov-file).\n",
    "\n",
    "Once the dataset is downloaded, you can get it into FiftyOne format.\n",
    "\n",
    "First, you will need to \"unpack\" the json file that comes with the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "images_path = \"./colorswap\"\n",
    "\n",
    "# Load train.json\n",
    "with open('./colorswap/train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "# Load test.json\n",
    "with open('/Users/harpreetsahota/workspace/datasets/colorswap/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Combine the two datasets\n",
    "packed_annotations = train_data + test_data\n",
    "\n",
    "unpacked_annotations = []\n",
    "\n",
    "for item in packed_annotations:\n",
    "    unpacked_annotations.append({\n",
    "        \"image_path\": os.path.join(images_path, item[\"image_1\"]),\n",
    "        \"caption\": item[\"caption_1\"],\n",
    "        \"image_source\": item[\"image_source\"]\n",
    "    })\n",
    "    unpacked_annotations.append({\n",
    "        \"image_path\": os.path.join(images_path, item[\"image_2\"]),\n",
    "        \"caption\": item[\"caption_2\"],\n",
    "        \"image_source\": item[\"image_source\"]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our custom labels, we can write helper functions that will define the schema for the FiftyOne dataset object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.core.fields as fof\n",
    "import os\n",
    "\n",
    "def create_colorswap_dataset(name) -> fo.Dataset:\n",
    "\t\"\"\"\n",
    "\tCreates schema for a FiftyOne dataset.\n",
    "\t\"\"\"\n",
    "\tdataset = fo.Dataset(name=name, persistent=True, overwrite=True)\n",
    "\n",
    "\tdataset.add_sample_field(\n",
    "\t\t'prompt', \n",
    "\t\tfof.StringField,\n",
    "\t\tdescription='Prompt that generated image'\n",
    "\t\t)\n",
    "\n",
    "\tdataset.add_sample_field(\n",
    "\t\t'image_source', \n",
    "\t\tfof.StringField,\n",
    "\t\tdescription='Model that generated image'\n",
    "\t\t)\n",
    "\t\n",
    "\treturn dataset\n",
    "\n",
    "\n",
    "def create_fo_sample(image: dict) -> fo.Sample:\n",
    "    \"\"\"\n",
    "    Creates a FiftyOne Sample from a given image entry with metadata and custom fields.\n",
    "\n",
    "    Args:\n",
    "        image (dict): A dictionary containing image data including the path and other properties.\n",
    "\n",
    "    Returns:\n",
    "        fo.Sample: The FiftyOne Sample object with the image and its metadata.\n",
    "    \"\"\"\n",
    "    \n",
    "    filepath = image.get('image_path')\n",
    "    \n",
    "    if not filepath:\n",
    "        return None\n",
    "\n",
    "    prompt = image.get('caption')\n",
    "    image_source = image.get('image_source')\n",
    "\n",
    "    sample = fo.Sample(\n",
    "        filepath=filepath,\n",
    "        prompt=prompt,\n",
    "        image_source=image_source,\n",
    "    )\n",
    "\n",
    "    return sample\n",
    "\n",
    "def add_samples_to_fiftyone_dataset(\n",
    "\tdataset: fo.Dataset,\n",
    "\tsamples: list\n",
    "\t):\n",
    "\t\"\"\"\n",
    "\tCreates a FiftyOne dataset from a list of samples.\n",
    "\n",
    "\tArgs:\n",
    "\t\tsamples (list): _description_\n",
    "\t\tdataset_name (str): _description_\n",
    "\t\"\"\"\n",
    "\tdataset.add_samples(samples, dynamic=True)\n",
    "\tdataset.add_dynamic_sample_fields()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load it into FiftyOne format, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_colorswap_dataset(\"colorswap\")\n",
    "\n",
    "samples = [create_fo_sample(image) for image in unpacked_annotations]\n",
    "\n",
    "add_samples_to_fiftyone_dataset(dataset, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "color_swap = fo.load_dataset(\"colorswap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this dataset is also available in FiftyOne format on the HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.utils.huggingface as fouh\n",
    "\n",
    "color_swap = fouh.load_from_hub(\n",
    "    \"Voxel51/ColorSwap\",\n",
    "    name=\"colorswap_full\",)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Concept Traversal Plugin for FiftyOne allows users to navigate the space of concepts in their dataset using both text and images. \n",
    "\n",
    "Key points:\n",
    "\n",
    "- You select a starting image from their dataset, then iteratively add text concepts with relative strengths to move around the multimodal embedding space.\n",
    "\n",
    "- Behind the scenes, it generates embedding vectors for the text prompts, combines them with the starting image vector, and performs a similarity search on the dataset.\n",
    "\n",
    "- Creating the plugin required generating a multimodal similarity index (e.g. using a CLIP model) on the dataset first.\n",
    "\n",
    "To use the plugin, a similarity index that supports prompts (i.e., embeds both text and images) must be present on the dataset. \n",
    "\n",
    "This can be created using the `fiftyone.brain` module, specifically the `compute_similarity` function, which takes the dataset, a `brain_key`, the `model_name` (e.g., `clip-vit-base32-torch`), and the `metric` (e.g., `cosine`) as arguments.\n",
    "\n",
    "The plugin can be installed by running the command `fiftyone plugins download https://github.com/jacobmarks/concept-interpolation`.\n",
    "\n",
    "The plugin provides two main operators:\n",
    "\n",
    "1. `open_interpolation_panel`: Opens the interpolation panel when clicked, but is only activated when the dataset has a similarity index.\n",
    "\n",
    "2. `interpolator`: Runs the actual interpolation between the two text prompts.\n",
    "\n",
    "In summary, this FiftyOne plugin enables users to explore the latent space between two text concepts by interpolating between their embeddings and visualizing the results, providing an interactive way to understand the relationships between different text prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embeddings...\n",
      " 100% |███████████████████| 38/38 [2.5s elapsed, 0s remaining, 15.7 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "sim_index = fob.compute_similarity(\n",
    "    color_swap,\n",
    "    brain_key=\"concept_embeddings\",\n",
    "    embeddings=\"clip_embeddings\",\n",
    "    model=\"clip-vit-base32-torch\",\n",
    "    metric=\"cosine\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating visualization...\n",
      "UMAP( verbose=True)\n",
      "Thu Aug 22 13:29:20 2024 Construct fuzzy simplicial set\n",
      "Thu Aug 22 13:29:20 2024 Finding Nearest Neighbors\n",
      "Thu Aug 22 13:29:21 2024 Finished Nearest Neighbor Search\n",
      "Thu Aug 22 13:29:22 2024 Construct embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f41c6e8ab9848b892405491e2ae46e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs completed:   0%|            0/500 [00:00]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  0  /  500 epochs\n",
      "\tcompleted  50  /  500 epochs\n",
      "\tcompleted  100  /  500 epochs\n",
      "\tcompleted  150  /  500 epochs\n",
      "\tcompleted  200  /  500 epochs\n",
      "\tcompleted  250  /  500 epochs\n",
      "\tcompleted  300  /  500 epochs\n",
      "\tcompleted  350  /  500 epochs\n",
      "\tcompleted  400  /  500 epochs\n",
      "\tcompleted  450  /  500 epochs\n",
      "Thu Aug 22 13:29:23 2024 Finished embedding\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<fiftyone.brain.visualization.VisualizationResults at 0x3e7b5b520>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fob.compute_visualization(\n",
    "    color_swap,\n",
    "    embeddings=\"clip_embeddings\",\n",
    "    method=\"umap\",\n",
    "    brain_key = \"umap_2d_clip\",\n",
    "    num_dims=2,\n",
    "    num_workers = os.cpu_count(),\n",
    "    progress=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(color_swap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
